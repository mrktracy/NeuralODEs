{"cells":[{"cell_type":"markdown","metadata":{"id":"h7yGT9eHVxoZ"},"source":["### Install or import dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14225,"status":"ok","timestamp":1702495440830,"user":{"displayName":"Parth Gajanan Ghayal","userId":"16879810634559733171"},"user_tz":300},"id":"JmgclNXt_52i","outputId":"a90871a1-9c1c-4eb9-d836-fdc4dae4d6f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.2.3\n"]}],"source":["try:\n","    import torchdiffeq\n","except ModuleNotFoundError:\n","    !pip install --quiet torchdiffeq\n","    import torchdiffeq\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchdiffeq as diff\n","from torchdiffeq import odeint\n","import csv\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.optim.lr_scheduler import StepLR, ExponentialLR\n","import time\n","import random\n","import os\n","import logging\n","\n","print(torchdiffeq.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3rx2trqkjwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702495462357,"user_tz":300,"elapsed":17572,"user":{"displayName":"Parth Gajanan Ghayal","userId":"16879810634559733171"}},"outputId":"fa39fb23-bee9-4fc2-8dd5-9bdf26ccce3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"4pmArLm8Cq16"},"source":["### Read dataset and split into training, validation, and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ehb0MAn0l21H"},"outputs":[],"source":["# filename = \"/content/drive/MyDrive/EC523 Final Project/Data/simulated_dataset-42_G=1_sumSteps=200.csv\"\n","\n","# # Read data\n","# with open(filename, 'r') as csvfile:\n","#     trajs = torch.tensor([[float(value) for value in row] for row in csv.reader(csvfile)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkvHfEna94KW"},"outputs":[],"source":["# # define function to round down to nearest 200\n","# def rounddown(x, number):\n","#   '''Rounds down to the nearest multiple of number specified'''\n","#   return int(np.floor(x / number)) * number\n","\n","# # split data into training, validation, and testing\n","# # trajectories are randomly initiated, so there is no need to further randomize\n","\n","# rows = trajs.shape[0]\n","# train_split = 0.7\n","# val_split = 0.15\n","\n","# train_stop = rounddown(rows*train_split, 200)\n","# val_stop = train_stop + rounddown(rows*val_split, 200)\n","\n","# train_data = trajs[:train_stop,:]\n","# val_data = trajs[train_stop:val_stop,:]\n","# test_data = trajs[val_stop:,:]\n","\n","train_file = \"/content/drive/MyDrive/EC523 Final Project/Data/simulated_dataset-42_G=1_sumSteps=200_TRAIN.csv\"\n","val_file = \"/content/drive/MyDrive/EC523 Final Project/Data/simulated_dataset-42_G=1_sumSteps=200_VAL.csv\"\n","test_file = \"/content/drive/MyDrive/EC523 Final Project/Data/simulated_dataset-42_G=1_sumSteps=200_TEST.csv\"\n","\n","# np.savetxt(train_file, train_data, delimiter=\",\")\n","# np.savetxt(val_file, val_data, delimiter=\",\")\n","# np.savetxt(test_file, test_data, delimiter=\",\")"]},{"cell_type":"markdown","metadata":{"id":"Yvn5nuJeVnLH"},"source":["### Define custom dataset for associating initial conditions with trajectories"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myz_sfYAViCB"},"outputs":[],"source":["# Define custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, filename, device, num_gpus, num_steps = 200):\n","        with open(filename, 'r') as csvfile:\n","\n","          # read the provided file\n","          self.trajs = torch.tensor([[float(value) for value in row] for row in csv.reader(csvfile)])\n","\n","        self.num_steps = num_steps\n","\n","    def __getitem__(self, idx):\n","\n","        # file divided into chunks of num_steps rows\n","        start_index = self.num_steps*idx\n","        end_index = start_index + self.num_steps\n","\n","        # columns 4 to end contain positions and velocities\n","        # take as input to the model the initial condition of the trajectory\n","        input = self.trajs[start_index,4:]\n","\n","        # as \"label\" take the whole trajectory\n","        label = self.trajs[start_index:end_index,4:]\n","\n","        return input, label\n","\n","    def __len__(self): # return length of dataset\n","        if len(self.trajs) % self.num_steps != 0:\n","            raise ValueError(\"Total number of rows in the dataset is not divisible by num_steps\")\n","\n","        length = len(self.trajs)//self.num_steps\n","        return length"]},{"cell_type":"markdown","metadata":{"id":"tEVuNV5ZWnrm"},"source":["### Define neural network model itself"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBOC87jYENlz"},"outputs":[],"source":["class NeuralODE(nn.Module):\n","    def __init__(self, input_dim, output_dim, reservoir_size=128):\n","        super(NeuralODE, self).__init__()\n","\n","        # Define the forward pass of the network\n","        self.f = nn.Sequential(\n","            nn.Linear(input_dim, reservoir_size),\n","            nn.ReLU(),\n","            nn.Linear(reservoir_size, reservoir_size),\n","            nn.ReLU(),\n","            nn.Linear(reservoir_size, output_dim)\n","        )\n","\n","    def forward(self, t, state):\n","\n","        # output represents the derivative of the state\n","        state_dot = self.f(state)\n","\n","        return state_dot\n"]},{"cell_type":"markdown","metadata":{"id":"pnCqvo0kZU2e"},"source":["### Define evaluation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4X_fInMZXXu"},"outputs":[],"source":["def evaluate_model(model, dataloader, device, max_batches = None):\n","\n","    # put in eval mode in case any dropout used, etc.\n","    model.eval()\n","    criterion = nn.MSELoss()\n","\n","    with torch.no_grad():\n","\n","        num_samples = 0\n","        total_error = 0\n","        for idx, (inputs, targets) in enumerate(dataloader):\n","\n","            # move input and targets to device\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            # forward pass\n","            # Define the time points\n","            timesteps = torch.linspace(0, 2, 200)\n","            timesteps = timesteps.to(device)\n","\n","            # Simulate the system using the neural ODE\n","            outputs = odeint(model, inputs, timesteps, method='dopri5')\n","            outputs = outputs.permute(1,0,-1)\n","\n","            loss = criterion(outputs,targets)\n","\n","            # increment total error and number of samples\n","            total_error += loss.item()*inputs.size(0)\n","            num_samples += inputs.size(0)\n","\n","            if max_batches is not None and idx + 1 == max_batches:\n","                break\n","\n","    return total_error / num_samples"]},{"cell_type":"markdown","metadata":{"id":"fI2pw866Wrdm"},"source":["### Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qrwHQNhEUPh"},"outputs":[],"source":["# use He initialization\n","def initialize_weights_he(m):\n","    if isinstance(m, nn.Linear):\n","        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","        if m.bias is not None:\n","            nn.init.constant_(m.bias, 0)\n","\n","def main():\n","\n","    # Define Hyperparameters\n","    EPOCHS = 100\n","    BATCH_SIZE = 32\n","    LEARNING_RATE = 1e-3\n","    LOGS_PER_EPOCH = 10\n","    BATCHES_PER_PRINT = 5\n","    EPOCHS_PER_SAVE = 10\n","    VERSION = \"base_G1_itr1\"\n","    VERSION_SUBFOLDER = \"\" # e.g. \"MNIST/\" or \"\"\n","\n","\n","    # Create log file\n","    logfile = f\"/content/drive/My Drive/EC523 Final Project/Results/runlog_{VERSION}.log\"\n","    os.makedirs(os.path.dirname(logfile), exist_ok=True)\n","\n","    seed = 42\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    # Initialize device, model\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    num_gpus = torch.cuda.device_count()\n","\n","    # Instantiate the neural ODE model\n","    model = NeuralODE(input_dim=12, output_dim=12, reservoir_size=128).to(device)\n","\n","    # initialize weights\n","    model.apply(initialize_weights_he)\n","\n","    if num_gpus > 1:  # use multiple GPUs\n","        model = nn.DataParallel(model)\n","\n","    # Load saved model\n","    # state_dict_saved = torch.load('../modelsaves/[NEEDS FILENAME].pth')\n","    # model.load_state_dict(state_dict_saved)\n","\n","    # intantiate datasets\n","    train_dataset = CustomDataset(filename = train_file, num_steps = 200, device = device, num_gpus = num_gpus)\n","    val_dataset = CustomDataset(filename = val_file, num_steps = 200, device = device, num_gpus = num_gpus)\n","\n","    # Instantiate data loaders, optimizer, criterion\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    train_length = len(train_dataloader)\n","    batches_per_log = train_length // LOGS_PER_EPOCH\n","\n","    optimizer = torch.optim.Adam(list(model.parameters()),\n","                                lr=LEARNING_RATE)\n","\n","    scheduler = ExponentialLR(optimizer, gamma=0.98)\n","    criterion = nn.MSELoss()\n","\n","    # Training loop\n","    for epoch in range(EPOCHS):\n","        avg_loss = 0\n","        count = 0\n","        for idx, (inputs, targets) in enumerate(train_dataloader):\n","\n","            # begin timing\n","            if idx % BATCHES_PER_PRINT == 0:\n","                start_time = time.time()\n","\n","            # move inputs and targets to GPU\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            # Define the time points for solving ODE\n","            timesteps = torch.linspace(0, 2, 200)\n","            timesteps = timesteps.to(device)\n","\n","            # Simulate the system using the neural ODE\n","            outputs = odeint(model, inputs, timesteps, method='dopri5')\n","            outputs = outputs.permute(1,0,-1)\n","\n","            loss = criterion(outputs,targets)\n","\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            avg_loss += loss.item()\n","            count += 1\n","\n","            # print timing to window\n","            if (idx+1) % BATCHES_PER_PRINT == 0:\n","                end_time = time.time()\n","                batch_time = end_time - start_time\n","                print(f\"\\r{BATCHES_PER_PRINT} batches processed in {batch_time:.2f} seconds. Training loss: {avg_loss/count:.6f}\", end=\"\")\n","\n","            # calculate validation loss, write to logging\n","            if (idx+1) % batches_per_log == 0:\n","                val_loss = evaluate_model(model, val_dataloader, device, max_batches=150)\n","                output = f\"Epoch {epoch+1} - {idx+1}/{train_length}. loss: {avg_loss/count:.4f}. lr: {scheduler.get_last_lr()[0]:.6f}. val: {val_loss:.6f}\"\n","                print(\"\\r\"+output, end=\"\")\n","                with open(logfile, \"a\") as file:\n","                  file.write(output+\"\\n\")\n","                avg_loss = 0 # reset counters\n","                count = 0\n","\n","        # save model\n","        if (epoch+1) % EPOCHS_PER_SAVE == 0:\n","            save_file = f\"/content/drive/My Drive/EC523 Final Project/Results/modelsaves/{VERSION}/{VERSION_SUBFOLDER}{VERSION}_ep{epoch + 1}.pth\"\n","            os.makedirs(os.path.dirname(save_file), exist_ok=True)\n","            torch.save(model.state_dict(), save_file)\n","\n","        scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ci_M9ypkHzMo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e13d6f89-369b-45d2-fd5b-4c2a1732538b"},"outputs":[{"output_type":"stream","name":"stdout","text":["5 batches processed in 18.22 seconds. Training loss: 0.316055"]}],"source":["main()"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}